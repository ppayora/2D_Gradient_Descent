{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Readme.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AXyi7soNI7DT","colab_type":"text"},"source":["### Description\n","This project is the first part of SMM284 An introduction to machine learning 2019 CASS Business School. The goal of this project is to create a 2D gradient descent class, try to optimize the fucntion, and draw a conclusion on the effect of each hyperparameter with the result. Three techniques being used are 1. Plain Vanilla 2. Momentum 3. Nesterov and the function that is being tested is **SIX-HUMP CAMEL**"]},{"cell_type":"markdown","metadata":{"id":"03eQrWa4K9D2","colab_type":"text"},"source":["**Why is gradient descent so important in machine learning**\n","\n","Gradient descent is an important  because every machine learning algorithms have their own cost function. Cost function is a function that  measure how well the model perform.The lower it's the better.Therefore,we try to minimize so that the model have as much predictive power as possible."]},{"cell_type":"markdown","metadata":{"id":"b5LN5P96MLKt","colab_type":"text"},"source":["**Plain Vanilla Gradient descent**\n","\n","Plain Vanilla GD works by moving against the direction of gradient at the initial point of the function until the magnitude of gradient is within limited tolerance"]},{"cell_type":"markdown","metadata":{"id":"gI2ny-2BNZHf","colab_type":"text"},"source":["**Two modifications on Vanilla gradient descent**\n","\n"," Stochastic Gradient Descent \n","\n","  This method instead of using entire data set before moving by a step size(learning rate) uses one set of independent variables to update its parameters which is appropriate for big data.\n","\n","Application :\n","\n","1. Online learning where there is a continuous stream of data.\n","2. Map reduce and data parallelism\n","3. Product search\n","\n","Mini Batch Gradient Descent.\n","\n","  This method modify is different from gradient descent by instead of using just one dataset to minimize the cost function. it uses 2-100 datasets to update the parameter. Minibatch technique tends to work faster because of the vectorization in computation."]},{"cell_type":"markdown","metadata":{"id":"0UVNT9k9Nxau","colab_type":"text"},"source":["http://www.sfu.ca/~ssurjano/camel6.html\n","\n","Six-Hump Camel Fucntion\n","\n","$$f(x) = (4-2.1x_1^2 +\\frac{x_1^4}{3})x_1^2 +x_1x_2 + (-4+4x_2^2)x_2^2$$\n"]},{"cell_type":"markdown","metadata":{"id":"uUeFr3VyPGEY","colab_type":"text"},"source":["# Result Discussion"]},{"cell_type":"markdown","metadata":{"id":"A7ME2wlaPKnu","colab_type":"text"},"source":["Number of iteration vs Learning rate\n","\n","Generally, the number of iteration tend to decrease when the learning rate increase. However, when the learning rate is too high, the loss function can explode.  "]},{"cell_type":"markdown","metadata":{"id":"EStS9tizebnn","colab_type":"text"},"source":["**Momentum and Nesterov**\n","\n","Problem of Gradient Descent\n","  \n","\n","1.   Slow when gradient is constantly small\n","2.   Follow wrong path in Noisy gradient\n","\n","Particulary for this function if the learning rate is too high the magnitude of gradient is going to explode because it's very flat at the bottom and very steep on the side.\n","\n","Solution: Use the learning rate decay which decrease the learning rate every n steps which generally do better. Momentum and Nesterov allows me to increase the learning rate more than 10 times.\n","\n","ref: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VrP6B2-fgE7k","colab_type":"text"},"source":["**Further Improvement**\n","\n","1. Try different active learning rate technique\n","2. Try different gradient descent technique"]}]}